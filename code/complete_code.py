# import libraries
import pandas as pd
import numpy as np
import pickle
import multiprocessing as mp
import concurrent.futures
import time
from pycaret.regression import *

# read the data
training_data = pd.read_csv("training_data.csv")
test_data = pd.read_csv("test_data.csv")

# get the unique list of stores
unique_list_of_stores = training_data["Store"].unique()


# get the shared dictionary
shared_dictionary = mp.Manager().dict()

# create a placeholder to store predictions
# we will be appending predictions into this placeholder
# so we need to keep the column names same as test data plus one extra
# column (Label) at the end. Label is a column that gets generated by
# PyCaret when we use predict model function
predictions_place_holder = pd.DataFrame(columns=test_data.columns.insert(20, "Label"))
# add a column named "Label" to hold predictions
predictions_place_holder.to_csv("predictions_place_holder.csv", index=False)

# Create the training (target) function
def training(store):
    # slice the data by store, for training and test data
    training_df_for_a_store = training_data[training_data["Store"] == store]
    test_df_for_a_store = test_data[test_data["Store"] == store]

    # prepare for the PyCaret's setup function
    s = setup(
        data=training_df_for_a_store,
        target="Sales",
        fold_strategy="timeseries",
        ignore_features=["Store"],
        # n_jobs controlls PyCaret's internal multiprocessing
        # we need to set it to one, since we are already using
        # all cores for our own multiprocessing
        n_jobs=1,
        silent=True,
        verbose=False,
        session_id=123,
    )

    # run compare models function
    m = compare_models(sort="mape", verbose=False)

    # predict with the selected model, it returns a df
    prediction_for_a_store = predict_model(m, data=test_df_for_a_store)

    # append the predictions to our place holder
    prediction_for_a_store.to_csv(
        "predictions_place_holder.csv", mode="a", index=False, header=False
    )

    # we can now save the preprocessign pipeline
    # but we first need to attach the trained model to
    # once we do that, we can simply save that to our shared dictionary

    # get the preprocessing pipeline using get_config function
    pipeline_object = get_config("prep_pipe")
    # add the model to the pipeline object
    pipeline_object.steps.append(["trained_model", m])
    # save it to shared dictionary
    shared_dictionary[store] = pipeline_object

    return None


# run the transformation in parallel
if __name__ == "__main__":

    s = time.perf_counter()

    with concurrent.futures.ProcessPoolExecutor() as executor:
        result = executor.map(
            training,
            unique_list_of_stores,
        )

    # save the shared dictionary to the cwd
    d = {}
    d = shared_dictionary.copy()
    pickle.dump(d, open("saved_models.p", "wb"))

    e = time.perf_counter()

    print(f"parallel took {e-s:0.3f} sec to run ")
